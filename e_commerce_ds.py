# -*- coding: utf-8 -*-
"""E-Commerce_DS.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EWsMMHcWfZz6K5rxMI6cXYAMulFO9uFv

# BODEGA SPEEDY PROJECT

---

**Fecha de creación:**

- 18 de Febrero 2024

**Integrantes del Equipo:**

- Machine Learning Engineer [Julio César Martínez](https://www.linkedin.com/in/juliocmi/)
- Data Scientist [Lola Miranda Díaz](https://www.linkedin.com/in/lola-diaz-8606-/)
- Data Engineer [Álvaro Peña](https://www.linkedin.com/in/aalvarop-pe/)
- BI Analyst [Luis Alberto Chacón](https://www.linkedin.com/in/luisalbertochacon/)

---

## Tabla de Contenido

1. Importar librerías
2. Cargar los Datos
3. Limpieza de Datos
4. EDA - Análisis Exploratorio de Datos
5. Análisis Estadístico de Datos
6. Requerimientos de Marketing
7. Requerimientos de Finanzas

---

## Objetivo

En el dinámico y competitivo mundo del **comercio electrónico**, la capacidad de comprender y anticipar las preferencias y comportamientos de los consumidores es fundamental para el éxito empresarial. En este contexto, el **aprendizaje automático** ha emergido como una herramienta poderosa que puede proporcionar **insights** valiosos para las empresas de e-commerce.

Este **proyecto** se enfoca en la aplicación de técnicas de clasificación de **machine learning** en el ámbito del **comercio electrónico**, con el objetivo de **mejorar** la personalización de la experiencia del usuario, optimizar las estrategias de marketing y aumentar las tasas de conversión.

Exploraremos cómo los algoritmos de clasificación pueden analizar datos transaccionales, historiales de navegación, interacciones en redes sociales y otros factores relevantes para segmentar a los clientes, predecir sus preferencias y comportamientos futuros, y recomendar productos de manera más precisa y efectiva. Al hacerlo, este proyecto busca no solo mejorar la eficiencia y rentabilidad de las empresas de e-commerce, sino también mejorar la satisfacción y fidelización de los clientes al ofrecer experiencias de compra más personalizadas y relevantes.

---

## Etiquetar los Datos

Para etiquetar los datos se realizó previamente un trabajo de EDA que nos permitió conocer los detalles de las columnas. Inicialmente verás diferentes los nombres de las columnas, sin embargo, en el proceso podrás ver los cambios que nos llevaron a tener las siguientes etiquetas.

`customer_id` - ID del cliente

`customer_code` - Código único del cliente

`customer_city` - Ciudad de residencia del cliente

`customer_state` - Estado de residencia del cliente

`order_status` - Estado de la orden de compra

`order_purchase_tt` - Fecha de la orden de compra

`order_approved` - Fecha en que fue aprobada la orden de compra

`carrier_date` - Fecha de transporte de mercancía

`customer_date` - Fecha del cliente

`order_estimated_delivery_date` - Fecha estimada de entrega

`shipping_limit_date` - Fecha límite de envío

`price` - Precio del producto comprado por el cliente

`freight_value` - Costos de envío según la residencia del cliente

`seller_zip_code_prefix` - Código postal del vendedor

`seller_city` - Ciudad en la que se ubica el vendedor

`seller_state` - Estado en el que se ubica el vendedor

`payment_sequential` - Secuencia de pago

`payment_type` - Tipo de pago

`payment_installments` - Cuotas de pago en mensualidades

`payment_value` - Valor del pago final (precio + envío)

`product_category_name` - Nombre de la categoría del producto

`product_description_lenght` - Longitud de la descripción del producto

`product_photos_qty` - Número de fotos del producto

`weight` - Peso del producto en gramos

`lenght` - Largo del producto en cm

`Height` - Alto del producto en cm

`r_score` - Puntuación de la reseña o comentario

`rc_title` - Título de la reseña o comentario

`rc_message` - Reseña o comentario

`rc_date` - Fecha de publicación de la reseña

`r_answer_tt` - Respuesta a la reseña

---

## Proyecto

### 1 Importar Librerías
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import seaborn as sns
import psycopg2
import warnings
import time
import statsmodels.api as sm

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split, GridSearchCV, TimeSeriesSplit
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor

from pylab import rcParams
from matplotlib import pyplot
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.graphics.tsaplots import plot_acf
from statsmodels.graphics.tsaplots import plot_pacf
from statsmodels.tsa.stattools import adfuller
from statsmodels.tsa.statespace.sarimax import SARIMAX
from lightgbm import LGBMRegressor

warnings.filterwarnings("ignore")                   # Ignorar warnings
pd.set_option('display.max_columns', None)          # Visualizar columnas
pd.set_option('display.max_rows', None)             # Visualizar filas
pd.options.display.float_format = "{:,.2f}".format  # Quitar la notación científica
np.set_printoptions(threshold=np.inf)               # Ver arrays



"""---

### 2 Cargar los Datos
"""

# ## CONEXIÓN A LA BASE DE DATOS
# ##--------------------------------
try:
    # Establecer conexión
    connection = psycopg2.connect(
        host='dpg-cnaoq1ed3nmc73dnhop0-a.oregon-postgres.render.com',
        port=5432,
        database='e_brazil',
        user='root',
        password='LOJYXEXM378CzmJtuzK9WNWo8RPY4v3z'
        )
    print("Conexión exitosa a la base de datos")
except psycopg2.Error as error:
    # Manejar el error
    print("Error al ejecutar la consulta:", error)
    # Hacer rollback para cancelar la transacción actual
    connection.rollback()

# DATABASE CONNECTION MARKETING OLAP
  # SCHEMA: Marketing
  # TABLE : Customers
cursor = connection.cursor()
cursor.execute("SELECT * FROM marketing.customers;")
rows = cursor.fetchall()

mk_customers = pd.DataFrame(rows, columns=['customer_id', 'customer_unique_id', 'customer_zip_code_prefix',
                                           'customer_city', 'customer_state'])

 # CONECCTION DATABASE
  # SCHEMA: Marketing
  # TABLE : Order_Items

cursor.execute("SELECT * FROM marketing.order_items;")
rows = cursor.fetchall()

cols = ['order_id', 'order_item_id', 'product_id', 'seller_id',
       'shipping_limit_date', 'price', 'freight_value']

mk_items = pd.DataFrame(rows, columns=cols)
mk_items.head()

  # CONECTION DATABASE
  # SCHEMA : Marketing
  # TABLE  : Order_Reviews

cursor.execute("SELECT * FROM marketing.order_reviews;")
rows = cursor.fetchall()

cols = ['review_id', 'order_id', 'review_score', 'review_comment_title',
       'review_comment_message', 'review_creation_date',
       'review_answer_timestamp']

mk_reviews = pd.DataFrame(rows, columns=cols)

# CONECTION DATABASE
  # SCHEMA : Marketing
  # TABLE  : Orders

cursor.execute("SELECT * FROM marketing.orders;")
rows = cursor.fetchall()

cols = ['order_id', 'customer_id', 'order_status', 'order_purchase_timestamp',
       'order_approved_at', 'order_delivered_carrier_date',
       'order_delivered_customer_date', 'order_estimated_delivery_date']

mk_orders = pd.DataFrame(rows, columns=cols)

  # CONECTION DATABASE
  # SCHEMA : Marketing
  # TABLE  : Products

cursor.execute("SELECT * FROM marketing.products;")
rows = cursor.fetchall()

cols = ['product_id', 'product_category_name', 'product_name_lenght',
       'product_description_lenght', 'product_photos_qty', 'product_weight_g',
       'product_length_cm', 'product_height_cm', 'product_width_cm']

mk_products = pd.DataFrame(rows, columns=cols)

## DATABASE CONECCTION FINANCE OLAP
##---------------------------------

 # CONECTION DATABASE
  # SCHEMA : Finance
  # TABLE  : Customers
cursor = connection.cursor()
cursor.execute("SELECT * FROM finance.customers;")
rows = cursor.fetchall()

cols = ['customer_id', 'customer_unique_id', 'customer_zip_code_prefix',
        'customer_city', 'customer_state']

fin_customers = pd.DataFrame(rows, columns=cols)

  # CONECTION DATABASE
  # SCHEMA : Finance
  # TABLE  : Order_Payments

cursor.execute("SELECT * FROM finance.order_payments;")
rows = cursor.fetchall()

cols = ['order_id', 'payment_sequential', 'payment_type',
       'payment_installments', 'payment_value']

fin_payments = pd.DataFrame(rows, columns=cols)

  # CONECTION DATABASE
  # SCHEMA : Finance
  # TABLE  : Orders

cursor.execute("SELECT * FROM finance.orders;")
rows = cursor.fetchall()

cols = ['order_id', 'customer_id', 'order_status', 'order_purchase_timestamp',
       'order_approved_at', 'order_delivered_carrier_date',
       'order_delivered_customer_date', 'order_estimated_delivery_date']

fin_orders = pd.DataFrame(rows, columns=cols)

## DATOS MARKETING
##------------------
mk_df = pd.merge(mk_customers, mk_orders)
mk_df = pd.merge(mk_df, mk_items)
mk_df = pd.merge(mk_df, mk_products)
mk_df = pd.merge(mk_df, mk_reviews)

mk_df.head()

## DATOS FINANZAS
##---------------
fn_df = pd.merge(fin_customers, fin_orders)
fn_df = pd.merge(fn_df, fin_payments)

fn_df.head()

df = pd.merge(mk_df, fn_df)
df.columns.tolist()

# Ver el set de datos completo
df.head()



"""---

### 3 Limpieza de Datos
"""

# Cuales son las dimensiones del set de datos DF
print(df.shape)

# Averiguar la información General
print(df.info())

# Cambia el nombre de las columnas a uno más sencillo y legible con excepción de los ID's
nombres_columnas = df.columns.tolist()
print(nombres_columnas)
nombres_columnas=[
    'customer_id',
    'customer_unique_id',
    'customer_code',
    'customer_city',
    'customer_state',
    'order_id',
    'order_status',
    'order_purchase_tt',
    'order_approved',
    'carrier_date',
    'customer_date',
    'order_estimated_delivery_date',
    'order_item_id',
    'product_id',
    'seller_id',
    'shipping_limit_date',
    'price',
    'freight_value',
    'product_category_name',
    'product_name_lenght',
    'product_description_lenght',
    'product_photos_qty',
    'product_weight_g',
    'product_length_cm',
    'product_height_cm',
    'product_width_cm',
    'review_id',
    'r_score',
    'rc_title',
    'rc_message',
    'rc_date',
    'r_answer_tt',
    'payment_sequential',
    'payment_type',
    'payment_installments',
    'payment_value'
    ]
df.columns = nombres_columnas

# Verificar si el tipo de datos es correcto para cada columna y hacer las correcciones pertinentes
# Object, Int, DateTime, Float
df['order_purchase_tt'] = pd.to_datetime(df['order_purchase_tt'])
df['order_approved']    = pd.to_datetime(df['order_approved'])
df['carrier_date']      = pd.to_datetime(df['carrier_date'])
df['customer_date']     = pd.to_datetime(df['customer_date'])
df['order_estimated_delivery_date'] = pd.to_datetime(df['order_estimated_delivery_date'])
df['shipping_limit_date'] = pd.to_datetime(df['shipping_limit_date'])
df['rc_date']           = pd.to_datetime(df['rc_date'])
df['r_answer_tt']       = pd.to_datetime(df['r_answer_tt'])
print(df.dtypes)

# Elimina todas las columnas de ID's excepto la de los clientes "customer_id"
delete_id = [col for col in df.columns if 'id' in col.lower() and col != 'customer_id']
print(delete_id) #lista de las columnas con id que no son costumer_id
df = df.drop(columns=delete_id)

# Enlista las variables categóricas
objetos_cat = df.select_dtypes(include=['object'])
int_cat = df.select_dtypes(include=['int64'])
float_cat = df.select_dtypes(include=['float64'])
print(objetos_cat.columns.tolist())
print(int_cat.columns.tolist())
print(float_cat.columns.tolist())

# Enlista las variables numéricas
var_numericas = df.select_dtypes(include=['int64','float64'])
print(var_numericas.columns.tolist())

# Verificar nuevo tamaño del dataset
df.shape

# Visualizar primeras columnas del nuevo dataset
df.head()

"""#### 3.1 Verificar errores en los datos de cada columna

customer_code
"""

df["customer_code"].unique()

"""customer_city"""

df["customer_city"].unique()

"""customer_state"""

df["customer_state"].unique()

"""order_status"""

df["order_status"].unique()

"""order_purchase_tt"""

df["order_purchase_tt"]

"""order_approved"""

df["order_approved"].unique()

"""carrier_date"""

df['carrier_date'].unique()

"""customer_date"""

df['customer_date'].unique()

"""order_estimated_derivery_date"""

df['order_estimated_delivery_date'].unique()

"""shipping_limit_date"""

df['shipping_limit_date'].unique()

"""price"""

df['price'].sample(15)

"""freight_value"""

df['freight_value'].sample(15)

#df['seller_city'].unique()

# df['seller_city'] = df['seller_city'].replace(
#     ['vendas@creditparts.com.br','ribeirao preto / sao paulo', 'arraial d\'ajuda (porto seguro)', 'carapicuiba / sao paulo', 'maua/sao paulo',
#      'sao paulo - sp', 'sp / sp', 'santo andre/sao paulo', 's jose do rio preto', '04482255', 'sbc/sp', 'andira-pr','sp', 'brasilia df',
#      'barbacena/ minas gerais', 'pinhais/pr', 'jacarei / sao paulo', 'sao paulo / sao paulo', 'rio de janeiro \\rio de janeiro', 'auriflama/sp',
#      'sao paulo sp', 'mogi das cruzes / sp', 'lages - sc', 'cariacica / es', 'rio de janeiro / rio de janeiro', 'angra dos reis rj', 'sao sebastiao da grama/sp',
#      'novo hamburgo, rio grande do sul, brasil', 'aguas claras df', 'sbc'],
#     ['ribeirao preto', 'ribeirao preto', 'arraial d\'ajuda', 'carapicuiba', 'maua',
#      'sao paulo', 'sao paulo', 'santo andre', 'sao jose do rio preto', 'eusebio', 'sao bernardo do campo', 'andira','sao paulo', 'brasilia',
#      'barbacena', 'pinhais', 'jacarei', 'sao paulo', 'rio de janeiro', 'auriflama',
#      'sao paulo', 'mogi das cruzes', 'lages', 'cariacica', 'rio de janeiro', 'angra dos reis', 'sao sebastiao da grama',
#      'novo hamburgo', 'aguas claras', 'sao bernardo do campo'])

#df['seller_city'].unique()

"""Algunos errores que encontramos en esta columna:

- correo electrónico en lugar de ciudad
- ciudad + estado se elimina el estado
- abreviaturas de ciudad se sustituye con el nombre completo de la ciudad

payment_sequential
"""

np.sort(df['payment_sequential'].unique())

"""payment_type"""

df['payment_type'].unique()

"""payment_installments"""

np.sort(df['payment_installments'].unique())

"""payment_value"""

df['payment_value'].sample(10)

"""r_score"""

np.sort(df['r_score'].unique())

"""rc_title"""

df['rc_title'].unique()

"""rc_message"""

df['rc_message'].sample(15)

"""rc_date"""

df['rc_date'].unique()

"""r_answer_tt"""

df['r_answer_tt'].unique()

"""#### 3.2 Revisión de valores ausentes en el dataset"""

# Recuento de columnas con valores ausentes
df.isna().sum()

"""rc_title"""

print('No de valores ausentes:', df['rc_title'].isna().sum())

"""Podemos ver que existen más de 100 mil opiniones que no colocaron título a su comentario. Vamos a rellenar esta información con la palabra *No title* para evitar tener valores ausentes."""

df['rc_title'] = df['rc_title'].fillna('No title')
print('No de ausentes:', df['rc_title'].isna().sum())

"""rc_message"""

print('No de ausentes:', df['rc_message'].isna().sum())

"""Esta cantidad de valores ausentes son opiniónes que no recibieron un comentario pero que almenos dejaron alguna calificación sobre el producto. Vamos a sustituir estos valores con la palabra *No Comment*"""

df['rc_message'] = df['rc_message'].fillna('No comment')
print('No de ausentes', df['rc_message'].isna().sum())

"""Para el resto de columnas vamos a eliminar los valores ausentes ya que estos no representan una distribución considerable dentro de nuestro set de datos. La variable que más NA tiene es `customer_date` vamos a calcular su distribución."""

nan_values = df['customer_date'].isna().sum()
total_data = df.shape[0]

distribution = nan_values / total_data * 100
print(f'Porcentaje de valores ausentes en el dataset: {distribution:.2f}%')

"""Como se puede observar el porcentaje de nulos es tan bajo que no afecta los análisis del dataset así que podemos eliminarlos sin mayor problema, hagamos la operación y comprobemos los datos nulos."""

## Resto de valores ausentes

a = 100*(df['order_approved'].isna().sum() / df.shape[0])
b = 100*(df['carrier_date'].isna().sum() / df.shape[0])
c = 100*(df['customer_date'].isna().sum() / df.shape[0])
d = 100*(df['product_category_name'].isna().sum() / df.shape[0])
e = 100*(df['product_name_lenght'].isna().sum() / df.shape[0])
f = 100*(df['product_description_lenght'].isna().sum() / df.shape[0])
g = 100*(df['product_photos_qty'].isna().sum() / df.shape[0])
h = 100*(df['product_weight_g'].isna().sum() / df.shape[0])
i = 100*(df['product_length_cm'].isna().sum() / df.shape[0])
j = 100*(df['product_height_cm'].isna().sum() / df.shape[0])

print(f'Distribución de ausentes en order_approved:{a:.2f}%')
print(f'Distribución de ausentes en carrier_date:{b:.2f}%')
print(f'Distribución de ausentes en customer_date:{c:.2f}%')
print(f'Distribución de ausentes en product_category_name:{d:.2f}%')
print(f'Distribución de ausentes en product_name_lenght:{e:.2f}%')
print(f'Distribución de ausentes en product_description_lenght:{f:.2f}%')
print(f'Distribución de ausentes en product_photos_qty:{g:.2f}%')
print(f'Distribución de ausentes en product_weight_g:{h:.2f}%')
print(f'Distribución de ausentes en product_length_cm:{i:.2f}%')
print(f'Distribución de ausentes en product_height_cm:{j:.2f}%')

# Llenar valores ausentes

df['order_approved'] = df['carrier_date'].fillna(df['order_approved'].mean())
df['carrier_date']   = df['carrier_date'].fillna(df['carrier_date'].mean())
df['customer_date']  = df['carrier_date'].fillna(df['customer_date'].mean())

df['product_category_name']      = df['product_category_name'].fillna('cama_mesa_banho')
df['product_name_lenght']        = df['product_name_lenght'].fillna(df['product_name_lenght'].median())
df['product_description_lenght'] = df['product_description_lenght'].fillna(df['product_description_lenght'].median())
df['product_photos_qty']         = df['product_photos_qty'].fillna(df['product_photos_qty'].median())
df['product_weight_g']           = df['product_weight_g'].fillna(df['product_weight_g'].median())
df['product_length_cm']          = df['product_length_cm'].fillna(df['product_length_cm'].median())
df['product_height_cm']          = df['product_height_cm'].fillna(df['product_height_cm'].median())

df.isna().sum()

"""#### 3.3 Revisión de Valores Duplicados"""

duplicados = df.duplicated().sum()
print(f'Cantidad de valores duplicados:{duplicados}')

df = df.drop_duplicates()
duplicados = df.duplicated().sum()

print(f'Cantidad actual de duplicados:{duplicados}')
print()
print('Dimensiones del set de datos:', df.shape)

"""**Primeras Conlusiones**

Durante la **limpieza de los datos** encontramos lo siguiente:

- Errores en algunos datos dentro de la columna `seller_city` que aloja datos de las ciudades donde residen los vendedores. Estos errores incluyen: nombres de estados, ciudad + estado y correos electrónicos.

- Cambiamos algunos nombres de columnas para hacerlos más sencillos de leer.

- Identificamos variables categóricas y variables numéricas.

- Eliminamos las columnas de ID que sobraban en nuestro set de datos.

- Eliminamos valores nulos.

- Eliminamos los valores duplicados.

- El tamaño final del set de datos es de: 103,027 filas x 32 columnas
"""



"""---

### 4 Análisis Exploratorio de Datos

En esta sección vamos a realizar un análisis de cada columna para dar ver si podemos dar respuesta a los requerimientos del departamento de marketing y finanzas, y de paso, encontrar datos que sean de interés para la compañía.

#### 4.1 Requerimientos de Marketing

- **Clientes que han realizado una Compra**

Para conocer a los clientes que han realizado una compra, vamos a explorar la columna `order_status` donde los clientes que han realizado una compra deberían tener el estatus de **delivered** o entregado, mientras que los clientes que no realizaron una compra debería tener la leyenda **canceled**.
"""

df['customer_code'].nunique()

# Recuento de ordenes disponibles
df['order_status'].value_counts()

"""Vamos a crear una nueva clasificación para identificar si los clientes realizaron una compra o no."""

## Clasificar Status

def clasificar_status(status):
    if status in ['delivered', 'shipped', 'invoiced', 'processing', 'approved']:
        return 'yes'
    else:
        return 'no'

df['made_purchase'] = df['order_status'].apply(clasificar_status)

df.head()

# Estadísticas de las ciudades donde habitan los clientes
df['customer_city'].describe()

# Crear dataframe con los clientes que realizaron una compra
compras = df[['customer_code', 'customer_city', 'made_purchase']]
compras = compras[compras['made_purchase'] == 'yes']
compras = compras.reset_index(drop=True)
compras.head()

# Agrupar clientes por ciudad
group_city = compras.groupby(
    ['customer_city']).agg(
        {'customer_code':'count'}).sort_values(
            by='customer_code')
group_city.tail(15)

## Crear top 10 de ciudades con más clientes
top_10 = group_city.sort_values(by='customer_code', ascending=False).head(10)
top_10

# Crear y clasificar el resto de clientes de otras ciudades como 'others'
df_otros = group_city.drop(index=top_10.index.values)
df_otros['ciudades'] = 'others'
df_otros

df_otros_gpd = df_otros.groupby(
                                'ciudades').agg(
                                {'customer_code':'sum'}
                                ).reset_index()
df_otros_gpd

top_10 = top_10.reset_index()
top_10

df_top_10 = pd.concat([top_10, df_otros_gpd])
df_top_10 = df_top_10.set_index('ciudades')
x = df_top_10 / df_top_10.sum()*100
print(f'Porcentaje de Clientes por Ciudad,\n{x}')

ax= df_top_10.plot.pie(
    y='customer_code',
    startangle=90,
    autopct='%1.2f%%',
    colormap='GnBu',
    figsize=(9,9)
)

ax.set_ylabel('')
ax.legend(
    title='Ciudades con mayor cantidad de clientes',
    fontsize=10,
    bbox_to_anchor=(1,1)
)
ax.set_title('Porcentaje de Clientes por Ciudad de Procedencia', fontsize=15);

"""**Insights Encontrados**

- Existen **104,530 clientes** que han realizado al menos una compra.
- Los productos que se vendieron se distribuyen en 4,071 ciudades de las 5,570 que hay en Brasil representando el **73% de participación** en el país.
- Las ciudad que más compra realiza es **Sao Paulo** que posee el 15.68% de los clientes de la compañía.
- **El top 5 de ciudades** con más clientes son:
  - Sao Paulo
  - Rio de Janeiro
  - Belo Horizonte
  - Brasilia
  - Curitiba
- El **65% de los clientes** se distribuyen en el resto de ciudades de Brasil.

---

- **Productos con Mayor Nivel de Satisfacción**
"""

df['product_category_name'].describe()

df_products = df.pivot_table(
    columns=[
        'product_category_name'
        ],
    values='r_score').T.sort_values(
        by='r_score',
        ascending=False
        )
df_products

top_10_prod = df_products.head(10)
print(top_10_prod)
top_10_prod.plot.bar(
    figsize=(15,5),
    color='blue'
    )
plt.title('Top 10 Productos Mejor Calificados',
          fontsize=15)
plt.xlabel('Category', fontsize=12)
plt.ylabel('Score Points', fontsize=12)
plt.xticks(rotation=45)
plt.show();

last_10_prod = df_products.tail(10)
print(last_10_prod)
last_10_prod.plot.bar(
    figsize=(15,5),
    color='blue'
    )
plt.title('Los 10 Peores Productos',
          fontsize=15)
plt.xlabel('Category', fontsize=12)
plt.ylabel('Score Points', fontsize=12)
plt.xticks(rotation=45)
plt.show();

"""**Encontramos los siguientes Insigths**

- La compañía ofrece **73 tipos de productos** diferentes.
- El producto que más valoran los clientes es la ropa infantíl y juvenil.
- La categoría menos valorada por los clientes corresponde a los seguros y servicios.
"""



"""---

**Tendencia de Ventas en los Últimos 2 Años**
"""

fecha_inicio = df['order_approved'].min()
fecha_final = df['order_approved'].max()

print(f'Fecha inicial de ventas:      {fecha_inicio}')
print(f'Fecha más reciente de ventas: {fecha_final}')

trend_year = df[['customer_id','order_approved','payment_value','product_category_name']]
trend_year['year'] = trend_year['order_approved'].dt.year
trend_year['month'] = trend_year['order_approved'].dt.month

trend_year.to_csv('ventas_años.csv', encoding='utf-8')

df_2016 = trend_year[trend_year['year'] == 2016]
df_2017 = trend_year[trend_year['year'] == 2017]
df_2018 = trend_year[trend_year['year'] == 2018]

df_2016.to_csv('df_2016.csv', encoding='utf-8')

df_2016 = df_2016.groupby(['month','product_category_name']).agg({'payment_value':'sum'})
df_2016 = df_2016.sort_values(by=['month','payment_value'])
print(df_2016)

df_2016.plot.bar(title='Ventas de Productos en 2016 por Mes',
             figsize=(10,4),
             color='b',
             grid=True)
plt.xticks(rotation=90)
plt.xlabel('Mes / Categoría de Producto', fontsize=12)
plt.ylabel('Recaudación en Reales Brasileños',fontsize=12)
plt.show();

df_2017 = df_2017.groupby(['month','product_category_name']).agg({'payment_value':'sum'})
df_2017 = df_2017.sort_values(by=['month','payment_value'])
print(df_2017)

df_2017.plot(title='Ventas de Productos en 2017',
             figsize=(16,4),
             style='g--')
plt.xticks(rotation=90, fontsize=7)
plt.xlabel('Mes / Categoría de Producto', fontsize=12)
plt.ylabel('Recaudación en Reales Brasileños',fontsize=12)
plt.show();

df_2018 = df_2018.groupby(['month','product_category_name']).agg({'payment_value':'sum'})
df_2018 = df_2018.sort_values(by=['month','payment_value'])
print(df_2018)

df_2018.plot(title='Ventas de Productos en 2018',
             figsize=(16,4),
             style='g--')
plt.xticks(rotation=90, fontsize=7)
plt.xlabel('Mes / Categoría de Producto', fontsize=12)
plt.ylabel('Recaudación en Reales Brasileños',fontsize=12)
plt.show();

"""**Se encontraron los siguientes Insights**

- La base de datos de venta de productos abarca desde Octubre de 2016 hasta Agosto de 2018.
- Los productos más vendidos en Octubre 2016:
  - Muebles decorativos
  - Perfumería
  - Belleza y salud
  - Juguetes
  - Consolas de Videojuegos
- Los productos más vendidos en 2017:
  - Cama, mesa baño
  - Relojes presentes
  - Informatica accesorios
  - Muebles decorativos
  - Belleza y salud
- Los productos más vendidos en 2018:
  - Belleza y salud
  - Relojes presentes
  - Cama, mesa, baño
  - Utilidades domésticas
  - Muebles decorativos
- Analizando estas tendencias y los rankings anteriores descubrimos que estos productos tienen rankings entre 4 y 3.5 eso nos indica que no son los mejores valorados, sin embargo, sí son los que más tendencia tienden a comprar los clientes. Existen diversos factores por lo que este fenomeno se puede dar, quizás estos productos sean más accesibles al cliente por su precio, también puede deberse a los resultados de una campaña de ofertas en ciertas temporadas.
"""



"""---

#### 4.2 Requerimientos de Finanzas

- **Recaudación total de los productos en el último año**
"""

finance_df = df[['customer_id',
                 'order_approved',
                 'product_category_name',
                 'price',
                 'freight_value',
                 'payment_value',
                 'payment_type',
                 'payment_installments'
                 ]]
finance_df.head()

total_sells = finance_df.groupby(
    'product_category_name'
    ).agg(
        {'freight_value':'sum'}
        )
total_sells = total_sells.sort_values(
    by='freight_value',
    ascending=False
    )
total_sells.head(10)

total_sells.to_csv('total_ventas.csv', encoding='utf-8')

total_sells.sort_values(
    by='freight_value').plot.barh(
        figsize=(12,14),
        color='b'
        )

for i, value in enumerate(total_sells.sort_values(by='freight_value')['freight_value']):
    plt.text(value, i, f'R${value}', fontsize=7, va='center', color='b')

plt.title(
    'Recaudación total por producto en 2018',
    fontsize=16)
plt.xlabel(
    'Precio en Reales Brasileños',
    fontsize=12)
plt.ylabel(
    'Producto/Categoría',
    fontsize=12)
plt.show();

"""- **Método de Pago Más Utilizado**"""

finance_df['payment_type'].value_counts()

finance_df['payment_type'].value_counts().plot.bar(figsize=(12,6))
plt.xticks(rotation=25)
plt.title('Método de Pago Más Utilizado')
plt.xlabel('Forma de Pago')
plt.ylabel('No de Clientes')
plt.show();

"""- **Existencia de clientes deudores**"""

finance_df['payment_installments'].value_counts().sort_values(ascending=False).plot.bar()
print(finance_df['payment_installments'].value_counts().sort_values(ascending=False))
plt.title('Clientes que tienen deuda')
plt.xlabel('Mensualidades a pagar')
plt.ylabel('No de Clientes')
plt.xticks(rotation=45)
plt.show();

finance_df.to_csv('finance_df.csv', encoding='utf-8')

"""**Conclusiones**

- Los productos de cama, mesa, baño, belleza y salud, así como de deportes son los que más se venden y por lo tanto, los que más recaudación logran.

- El método de pago más utilizado es la tarjeta de crédito.

- El total de clientes registrados en 2018 tienen una deuda de al menos 1 mensualidad y los clientes que más deuda tienen abarcan desde los 12 meses hasta los 2 años de deuda.
"""



"""---

### 5 Análisis Estadístico

En esta sección vamos a estudiar y visualizar la correlacion que existe entre variables y crearemos nuevas categorías para mejorar la precisión de nuestro modelo de machine learning. Comenzaremos creando una clasificación adicional para los puntajes en la columna `r_score` que nos ayudará a determinar el nivel de satisfacción del cliente.
"""

## Creando funciones de clasificaciones

def quantile_classifier(quantile):
  if quantile < 3:
    return 'low'
  elif quantile < 4:
    return 'medium'
  else:
    return 'high'

def deuda_classifier(deuda):
  if deuda > 0:
    return 'Yes'
  else:
    return 'No'

def strip_spaces(value):
  if isinstance(value,str):
    return value.strip()
  else:
    return value

df['satisfaction_lvl'] = df['r_score'].apply(quantile_classifier)
df['debtor_customer'] = df['payment_installments'].apply(deuda_classifier)
df['quantile_pay'] = pd.qcut(df['payment_value'], q=[0,0.25,0.75,1], labels=['low', 'medium', 'high'])

df.head()

## Eliminar variables trabajando con una copia del dataset
data = df.copy()

dates = ['order_purchase_tt', 'order_approved', 'carrier_date',
         'customer_date', 'order_estimated_delivery_date',
         'shipping_limit_date','rc_date','r_answer_tt']
codes = ['customer_id','customer_code','product_name_lenght','product_description_lenght',
         'product_weight_g','product_length_cm','product_height_cm', 'rc_title', 'rc_message']

data = data.drop(columns=dates)
data = data.drop(columns=codes)

#data = data.applymap(strip_spaces)
data.head()

"""#### 5.1 Variables Categóricas"""

objetos_cat = data.select_dtypes(include=['object'])
cat_data    = objetos_cat.columns

var_numericas = data.select_dtypes(include=['int64','float64'])
num_data      = var_numericas.columns

cat_data, num_data

for col in cat_data:
  print(f'{col}:{objetos_cat[col].nunique()}')

a = 4
b = 2
c = 1

fig = plt.subplots(figsize=(20,25))

sns.set_style('whitegrid')
for i in objetos_cat:
    plt.subplot(a, b, c)
    plt.title(i, fontsize=50)
    plt.ylabel('%')
    plt.xlabel('category')
    objetos_cat[i].value_counts(normalize=True).head(40).plot(
        kind='bar',
        title=i,
        rot=90,
        edgecolor='black',
        cmap='plasma',
        linewidth=0.8
    )

    c = c+1
    plt.xticks(fontsize = 10)


plt.suptitle('Distribución de Variables Categóricas', fontsize='xx-large')
plt.tight_layout()
plt.show();



"""#### 5.2 Variables Numéricas"""

for col in var_numericas:
  print(f'{col}: {var_numericas[col].mean():.3f}')

var_numericas.describe()

a = 6
b = 2
c = 1

fig = plt.subplots(figsize=(15,18))

sns.set_style('whitegrid')
for i in var_numericas:
    plt.subplot(a, b, c)
    plt.title(i, fontsize=36)
    var_numericas[i].plot(
        kind='hist',
        title=i,
        bins=16,
        edgecolor='black',
        color='red',
        linewidth=0.8
    )

    c = c+1
    plt.xticks(fontsize = 8)


plt.suptitle('Distribución de Variables Numéricas', fontsize='xx-large')
plt.tight_layout
plt.show()

"""#### 5.3 Matriz de Correlación"""

sns.set_style('whitegrid')
plt.figure(figsize=(12,6))
sns.heatmap(
    data.corr(),
    annot=True,
    cmap='mako',
    linewidth=0.5)
plt.show();



"""---

### 6 Análisis Estocástico
"""

data = trend_year[trend_year['year'] == 2018]
data = data[['order_approved','customer_id']]
data = data.groupby('order_approved').agg({'customer_id':'count'})
data = data.reset_index()
data.columns = ['date', 'num_orders']
data.head(30)

print('Fecha de Incio:', data['date'].min())
print('Fecha de Finalización:', data['date'].max())

"""#### 6.1 Remuestreo"""

data = data.sort_index()
data = data.set_index('date').resample('D').sum()
data.head()

data.plot(figsize=(15, 3),color = 'blue')
plt.title('Ordenes por Día')
plt.ylabel('Frecuencia')
plt.xlabel('Fecha por Mes en 2018')
plt.show()

"""#### 6.2 Media Móvil y Desviación"""

data['mean'] = data['num_orders'].rolling(7).mean()
data['std']  = data['num_orders'].rolling(7).std()
print(data.head(10))

data.plot(figsize=(15, 3),color = ['darkblue','orange','green'])
plt.title('Ordenes de Compra por Día')
plt.ylabel('Frecuencia')
plt.xlabel('Fecha por Mes en 2018')
plt.show();

"""#### 6.3 Descomposición STL"""

rcParams['figure.figsize'] = 30, 20
decomposition2 = sm.tsa.seasonal_decompose(data['num_orders'], model='aditive')
fig = decomposition2.plot()
plt.show()

"""#### 6.4 Dick y Fuller Test"""

print ('Resultados del test de Dickey-Fuller:')
print ('------------------------------------------')
dftest = adfuller(data['num_orders'], autolag='AIC')
dfoutput = pd.Series(dftest[0:4],
                     index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])

print(dfoutput)

pyplot.figure(figsize=(12,9))

# ACF
#----
pyplot.subplot(211)
plot_acf(data['num_orders'], ax=pyplot.gca(), lags = 30)

#PACF
#----
pyplot.subplot(212)
plot_pacf(data['num_orders'], ax=pyplot.gca(), lags = 30)
pyplot.show()

"""---

### 7 Preparación de la Data
"""

data -= data.shift(fill_value=0)
data['mean'] = data['num_orders'].rolling(15).mean()
data['std'] = data['num_orders'].rolling(15).std()
print(data.head(10))
data.plot()

data_time = data.copy()
data_time.drop(['mean', 'std'], axis=1, inplace=True)

def make_features(data, max_lag, rolling_mean_size):
    # Agregar nuevas características
    data['year'] = data.index.year
    data['month'] = data.index.month
    data['day'] = data.index.day
    data['dayofweek'] = data.index.dayofweek

    # Agregar y calcular nuevos valores de desfase
    for lag in range(1, max_lag + 1):
        data['lag_{}'.format(lag)] = data['num_orders'].shift(lag)

    ## Agregar Media Móvil
    data['rolling_mean'] = data['num_orders'].shift().rolling(rolling_mean_size).mean()


# Aplicamos Función
#----------------------
make_features(data_time, 3, 3)

data_time = data_time.fillna(0)
data_time.head()

"""#### 7.1 Train Test Split"""

train, test = train_test_split(data_time, shuffle=False, test_size=0.1)

X_train = train.drop(['num_orders'], axis=1)
y_train = train['num_orders']
X_test  = test.drop(['num_orders'], axis=1)
y_test  = test['num_orders']

print("Set de Entrenamiento:", X_train.shape, y_train.shape)
print("Set de Prueba:", X_test.shape, y_test.shape)
print()

print("Fecha Inicial:", X_train.index.min(), "Fecha Final:", X_train.index.max())
print("Fecha Inicial:", X_test.index.min(), "Fecha Final", X_test.index.max())
print()

fig, ax = plt.subplots(figsize=(15,5))
y_train.plot(ax=ax)
y_test.plot(ax=ax)
ax.axvline('2018-08-17 00:00:00', color='black', ls='--')
plt.title('Train Test Split')
plt.xlabel('Fecha por Mes en 2018')
plt.ylabel('Frecuencia')
plt.legend(['training set', 'test set'])
plt.show();

"""### 8 Entrenamiento de Modelos

#### 8.1 Regresión Líneal
"""

tscv = TimeSeriesSplit(n_splits=252)

preds  = []
scores = []

for i,(train_idx, test_idx) in enumerate(tscv.split(data_time)):
    train = data_time.iloc[train_idx]
    test  = data_time.iloc[test_idx]

    features = ['year', 'month', 'day', 'dayofweek', 'lag_1', 'lag_2', 'lag_3', 'rolling_mean']
    target = 'num_orders'

    X_train = train[features]
    y_train = train[target]
    X_test = test[features]
    y_test = test[target]

    regresion_model = LinearRegression()
    regresion_model.fit(X_train, y_train)

    y_pred = regresion_model.predict(X_test)
    preds.append(y_pred)
    score = mean_squared_error(y_test, y_pred, squared=False)
    scores.append(score)
    print(f'score for fold {i}:', scores)

# Métrica Final
#--------------
RECM_lr = np.mean(scores)
print()
print('RECM Final:', RECM_lr)

"""#### 8.2 Random Forest"""

start_tr_rf = time.time()
rf_model = RandomForestRegressor(max_depth=15, n_estimators=2500, random_state=42)
rf_model.fit(X_train, y_train)
end_tr_rf = time.time()

# Calculamos las Predicciones
#----------------------------
start_ts_rf = time.time()
rf_predictions = rf_model.predict(X_test)
end_ts_rf = time.time()

# Establecemos el RECM
#----------------------------------------------------
RECM_rf = mean_squared_error(y_test, rf_predictions, squared=False)
print(f'RECM de Random Forest: {RECM_rf:.3f}')
print()
print(f'Tiempo de Entrenamiento: {end_tr_rf - start_tr_rf:.3f} seg')
print(f'Tiempo de Prueba: {end_ts_rf - start_ts_rf:.3f} seg')

"""#### 8.3 LightGBM Regressor"""

lightgbm_model   = LGBMRegressor(
    n_estimators = 2800,
    learning_rate= 0.01,
    num_leaves   = 31,
    random_seed  = 42,
    n_jobs       = -1
)

start_ltr = time.time()
lightgbm_model.fit(X_train, y_train)
end_ltr = time.time()

start_lts = time.time()
lgbm_predictions = lightgbm_model.predict(X_test)
end_lts = time.time()

RECM_lightgbm = mean_squared_error(y_test, lgbm_predictions, squared=False)
print(f'RECM de LightGBM: {RECM_lightgbm:.3f}')
print()
print(f'Tiempo de Entrenamiento: {end_ltr-start_ltr:.3f} seg')
print(f'Tiempo de Prueba: {end_lts-start_lts:.3f} seg')

"""### 9 Evaluación del Modelo

**Root Mean Square Error**

Una medida de uso frecuente de las diferencias entre los valores (valores de muestra o de población) predichos por un modelo o un estimador y los valores observados. La RECM representa la raíz cuadrada del segundo momento de la muestra de las diferencias entre los valores previstos y los valores observados o la media cuadrática de estas diferencias. Esta será la métrica base que utilizaremos para evaluar el rendimiento de nuestros modelos.

$$RECM = \sqrt ECM = \sqrt\frac{1}{n}\sum_{i=1}^{n} (\bar{y} - y)^2 $$
"""

models_table = pd.DataFrame({
    'modelo' : ['Regresión_Lineal', 'Random Forest', 'LightGBM', 'XGBoost'],
    'RECM' : [RECM_lr, RECM_rf, RECM_lightgbm, RECM_xgb]
})

models_table.sort_values(by='RECM')

models_table.sort_values(by='RECM', ascending=False).plot(kind='barh', color='royalblue')
plt.title('Resultados REMC')
plt.xlabel('Puntaje de la RECM')
plt.show();

